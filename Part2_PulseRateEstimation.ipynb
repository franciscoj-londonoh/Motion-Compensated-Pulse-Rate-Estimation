{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pulse Rate Estimation Algorithm\n",
    "\n",
    "### Contents\n",
    "- The Project description \n",
    "- The Code \n",
    "\n",
    "\n",
    "### Dataset\n",
    "The [Troika](https://ieeexplore.ieee.org/document/6905737) dataset is used to build the algorithm. \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Project description\n",
    "\n",
    "**Code description**: \n",
    "This code is implemented to process files containing PPG and (3D) accelerometer signals, and use them to train a regression model for predicting the heart rate of a subject with an estimation error below 10 BPMs.\n",
    "The code is divided in functions to facilitate its implementation and use. The code includes an accessing and loading stage of the signals, a preprocessing to extract relevant features and identify the targets. Then, all data is used to train a regression model, which is saved. To evaluate the algorithm, a cycle is put in place and each signal is used to estimate the error of the model.\n",
    "To run the complete code it is necessary to call the Evaluate function with the sample frequency as an argument. As an output, the mean BPM error estimation is printed, and the regression model is available as 'regression_model.joblib'. The following sections describe in more depth the data used to train and test the model, each function included in the code to process the data, built the model and test its accuracy, and the method used to assess the performance of the model. \n",
    "\n",
    "\n",
    "**Data description**: \n",
    "From the original files, one PPG signal and three-axis acceleration signals were used. The signals were recorded simultaneously from subjects age 18 to 35. 12 files were included and the PPG signals were recorded from wrist by two pulse oximeters with green LEDs (wavelength: 515nm). The acceleration signal was also recorded from wrist by a three-axis accelerometer. Both the pulse oximeter and the accelerometer were embedded in a wristband. All signals were sampled at 125Hz and sent to a nearby computer via Bluetooth. \n",
    "\n",
    "Each dataset with the similar name 'DATA_01_TYPE01' contains a variable 'sig'. The first row is a simultaneous recording of a PPG signal, which is recorded from the wrist of each subject. The last three rows are simultaneous recordings of acceleration data (in x-, y-, and z-axis). During data recording, each subject ran on a treadmill with changing speeds. \n",
    "\n",
    "    For datasets with names containing 'TYPE01', the running speeds changed as follows:\n",
    "        rest(30s) -> 8km/h(1min) -> 15km/h(1min) -> 8km/h(1min) -> 15km/h(1min) -> rest(30s)\n",
    "\n",
    "    For datasets with names containing 'TYPE02', the running speeds changed as follows:\n",
    "        rest(30s) -> 6km/h(1min) -> 12km/h(1min) -> 6km/h(1min) -> 12km/h(1min) -> rest(30s)\n",
    "\n",
    "For each dataset, the calculated \"ground-truth\" heart rate is provided, stored in the datasets with the corresponding name, say 'REF_01_TYPE01'. In each of this kind of datasets, there is a variable 'BPM0', which gives the BPM value in every 8-second time window. Note that two successive time windows overlap by 6 seconds. Thus, the first value in 'BPM0' gives the calcualted heart rate \"ground-truth\" in the first 8 seconds, while the second value in 'BPM0' gives the calculated heart rate \"ground-truth\" from the 3rd second to the 10th second.\n",
    "\n",
    "It is important to consider potential problems with the signal and the provided data, in general, such as the inclusion of incomplete information or incorrect information, for example due to biological and external noise sources, problems with the sensor and any device involved, etc. \n",
    "\n",
    "\n",
    "**Algorithhm description**: \n",
    "The algorithm is composed by several functions, organized in a sequence to load the original signals from stored files, processed these signals, trained a regression model and obtained predictive errors and confidence from test data. The functions building this algorithm are: \n",
    "\n",
    "* LoadTroikaDataset: Retrieve the .mat filenames for the troika dataset.\n",
    "* LoadTroikaDataFile: Load and extract signals from a troika data file. \n",
    "* GenerateFeatures: Extract features from the dataset. Generate features by sliding a window across each dataset and computing the features on each window. \n",
    "* Featurize: Featurization of the accelerometer signal.\n",
    "* LoadRegressor: Train and save a Decision Tree Regressor boost with an AdaBoost regressor, which is a meta-estimator that begins by fitting a regressor on the original dataset and then fits additional copies of the regressor on the same dataset but where the weights of instances are adjusted according to the error of the current prediction.\n",
    "* RunPulseRateAlgorithm: Load the trained regression model and process the data to calculate the error and confidence for each feature extracted from test signals.\n",
    "* AggregateErrorMetric: Computes an aggregate error metric based on confidence estimates.\n",
    "\n",
    "To obtain the PPG signals, a photoplethysmogram optically measures blood flow at the wrist. The LEDs in a PPG sensor shine a typically green light into your skin and your red blood cells absorb that green light. The reflected light is then measured by the photodetector. When your heart beats and blood perfuses through the wrist, there are more red blood cells that absorb the green light and the photodetector sees a smaller signal. As the heart fills back up with blood and blood leaves your wrist, the more green light is reflected back and the photodetector reading goes up. Ans is this oscillating waveform that have been used to calculate the pulse rate.\n",
    "\n",
    "The PPG signal is subject to different factors affecting the signal. Skin melanin content for example, impact the PPG signal since skin absorbs light from the LEDs as well and darker skin absorbs more light. This causes a DC shift and a reduction in SNR in the PPG signal for people with darker skin. The arm motion and position also affect the PPG signal due to the related movement of blood. Finger movement causes tendons and other structures to move, changing the sensor reading. Other potential sources of disturbance to the signal are malfunctioning of sensors, technical problems during the measurements, etc. Concretely, the project includes a pre-processing of the PPG signal, in which 3D acceleremeter data is used to partially \"clean\" the PPG data from the influence of movement and cadence of the arm during measurements. \n",
    "\n",
    "The main output of the algorithm (MAE) should be carefully assessed, taking into account potential problems (incorrect or incomplete information) or bias of the used data. Additionally, functions and parameters for each of them should be implemented correctly (format, data type, etc.) to avoid usage issues. \n",
    "\n",
    "**Algorithm performance**: \n",
    "The regresssion model was train/test using the LeaveOneGroupOut function, which provides train/test indices to split data according to a third-party provided group, in this case the original signal ID. This group information can be used to encode arbitrary domain specific stratifications of the samples as integers. The model is a decision tree regressor boost with an AdaBoost regressor, which is a meta-estimator that begins by fitting a regressor on the original dataset and then fits additional copies of the regressor on the same dataset but where the weights of instances are adjusted according to the error of the current prediction. As such, subsequent regressors focus more on difficult cases. Concretely, the model has 2000 estimators (the maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early) and a maximum depth of the tree of 20. \n",
    "Then, the algorithm calculates the errors (a numpy array of errors between pulse rate estimates and corresponding reference heart rates) and the confidence (a numpy array of confidence estimates for each pulse rate error). These two variables are further used to computes the aggregate error metric based on confidence estimates. A higher confidence means a better estimate, and the best 90% of the estimates are above the 10th percentile confidence. This value, which is the mean absolute error (MAE) at 90% availability, is the ultimate output of the algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "import scipy.signal\n",
    "from scipy.signal import savgol_filter\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "import joblib\n",
    "\n",
    "\n",
    "def LoadTroikaDataset():\n",
    "    \"\"\"\n",
    "    Retrieve the .mat filenames for the troika dataset.\n",
    "\n",
    "    Review the README in ./datasets/troika/ to understand the organization of the .mat files.\n",
    "\n",
    "    Returns:\n",
    "        data_fls: Names of the .mat files that contain signal data\n",
    "        ref_fls: Names of the .mat files that contain reference data\n",
    "        <data_fls> and <ref_fls> are ordered correspondingly, so that ref_fls[5] is the \n",
    "            reference data for data_fls[5], etc...\n",
    "    \"\"\"\n",
    "    data_dir = \"./datasets/troika/training_data\"\n",
    "    data_fls = sorted(glob.glob(data_dir + \"/DATA_*.mat\"))\n",
    "    ref_fls = sorted(glob.glob(data_dir + \"/REF_*.mat\"))\n",
    "    \n",
    "    return data_fls, ref_fls\n",
    "\n",
    "\n",
    "def LoadTroikaDataFile(data_fl, ref_fl):\n",
    "    \"\"\"\n",
    "    Loads and extracts signals from a troika data file.\n",
    "\n",
    "    Usage:\n",
    "        data_fls, ref_fls = LoadTroikaDataset()\n",
    "        ppg, accx, accy, accz = LoadTroikaDataFile(data_fls[0])\n",
    "\n",
    "    Args:\n",
    "        data_fl: (str) filepath to a troika .mat file.\n",
    "\n",
    "    Returns:\n",
    "        numpy arrays for ppg, accx, accy, accz signals.\n",
    "    \"\"\"\n",
    "    data = sp.io.loadmat(data_fl)['sig']\n",
    "    sig = data[2:]\n",
    "    ref = scipy.io.loadmat(ref_fl)[\"BPM0\"]\n",
    "        \n",
    "    return sig, ref\n",
    "\n",
    "\n",
    "def AggregateErrorMetric(pr_errors, confidence_est):\n",
    "    \"\"\"\n",
    "    Computes an aggregate error metric based on confidence estimates.\n",
    "\n",
    "    Computes the MAE at 90% availability. \n",
    "\n",
    "    Args:\n",
    "        pr_errors: a numpy array of errors between pulse rate estimates and corresponding \n",
    "            reference heart rates.\n",
    "        confidence_est: a numpy array of confidence estimates for each pulse rate\n",
    "            error.\n",
    "\n",
    "    Returns:\n",
    "        the MAE at 90% availability\n",
    "    \"\"\"\n",
    "    # Higher confidence means a better estimate. The best 90% of the estimates\n",
    "    #    are above the 10th percentile confidence.\n",
    "    percentile90_confidence = np.percentile(confidence_est, 10)\n",
    "\n",
    "    # Find the errors of the best pulse rate estimates\n",
    "    best_estimates = pr_errors[confidence_est >= percentile90_confidence]\n",
    "\n",
    "    # Return the mean absolute error\n",
    "    return np.mean(np.abs(best_estimates))\n",
    "\n",
    "\n",
    "def Evaluate(fs):\n",
    "    \"\"\"\n",
    "    Top-level function evaluation function.\n",
    "\n",
    "    Runs the pulse rate algorithm on the Troika dataset and returns an aggregate error metric.\n",
    "\n",
    "    Returns:\n",
    "        Pulse rate error on the Troika dataset. See AggregateErrorMetric.\n",
    "    \"\"\"\n",
    "    fs = fs\n",
    "    \n",
    "    # Retrieve dataset files\n",
    "    data_fls, ref_fls = LoadTroikaDataset()\n",
    "    errs, confs = [], []\n",
    "    \n",
    "    sub_index, signals, target = [], [], []\n",
    " \n",
    "    for i in range(len(data_fls)):\n",
    "        signal, reference = LoadTroikaDataFile(data_fls[i], ref_fls[i])\n",
    "        _id = data_fls[i].replace('./datasets/troika/training_data/', '') \n",
    "        sub_index.append(_id.replace('.mat', ''))\n",
    "        signals.append(signal)\n",
    "        target.append(reference)\n",
    "    \n",
    "    targets, sub_id, features, samples = GenerateFeatures(sub_index, signals, target, fs, 8, 2)  \n",
    "\n",
    "    LoadRegressor(2000, 20, features, targets, sub_id)\n",
    "\n",
    "    # Compute pulse rate estimates and estimation confidence\n",
    "    for data_fl, ref_fl in zip(data_fls, ref_fls):\n",
    "        error, confidence = RunPulseRateAlgorithm(data_fl, ref_fl)\n",
    "        errs.append(error)\n",
    "        confs.append(confidence)\n",
    "        print(data_fl)\n",
    "        # Compute aggregate error metric\n",
    "    \n",
    "    errs = np.hstack(errs)\n",
    "    confs = np.hstack(confs)\n",
    "\n",
    "    # Return per-estimate mean absolute error and confidence as a 2-tuple of numpy arrays\n",
    "    return AggregateErrorMetric(errs, confs)\n",
    "\n",
    "\n",
    "def RunPulseRateAlgorithm(data_fl, ref_fl):\n",
    "    \"\"\"\n",
    "    Load the trained regression model and process the data to calculate the error and confidence for each feature extracted \n",
    "    from test signals\n",
    "\n",
    "    Args:\n",
    "        data_fl: a list of all filepaths to troika .mat files containing the signal data\n",
    "        ref_fls: a list of all filepaths to .mat files containing reference data\n",
    "        \n",
    "        <data_fls> and <ref_fls> are ordered correspondingly, so that ref_fls[5] is the \n",
    "            reference data for data_fls[5], etc...\n",
    "        \n",
    "    Returns:\n",
    "        error: A numpy array containing the calculated error for each extracted feature\n",
    "        confid: A numpy array containing the calculated confidence for each extracted feature\n",
    "    \"\"\"  \n",
    "    fs = 125 # Sample frequency\n",
    "    error, confid = [], []\n",
    "    \n",
    "    signal, reference = LoadTroikaDataFile(data_fl, ref_fl)\n",
    "    _id = data_fl.replace('./datasets/troika/training_data/', '') \n",
    "    sub_index = _id.replace('.mat', '')\n",
    "    \n",
    "    targets, sub_id, features, samples = GenerateFeatures(sub_index, [signal], [reference], fs, 8, 2) \n",
    "\n",
    "    # Load the trained Random Forest Regressor model\n",
    "    regression = joblib.load('regression_model.joblib')\n",
    "\n",
    "    \n",
    "    for i, features in enumerate(features):\n",
    "        estimation = regression.predict(np.reshape(features, (1, -1)))\n",
    "          \n",
    "        ppg, accx, accy, accz = signal\n",
    "        ppg = BandpassFilter(ppg, fs, 3, (1.67, 3.17))\n",
    "        ppg = sp.signal.resample(ppg, 300*fs)\n",
    "        ppg = ppg[samples[i]: samples[i] + 1000]\n",
    "        \n",
    "        freqs_ppg = np.fft.rfftfreq(len(ppg), 1/fs)\n",
    "        fft_val = np.abs(np.fft.rfft(ppg))\n",
    "        ppg_peak_freq = freqs_ppg[np.argmax(fft_val)]\n",
    "        \n",
    "        fft_val[freqs_ppg <= 40/60.0] = 0.0\n",
    "        fft_val[freqs_ppg >= 240/60.0] = 0.0\n",
    "    \n",
    "        # Max magnitude frequency\n",
    "        est_fs = estimation / 60.0\n",
    "        fs_win = 30  / 60.0\n",
    "        fs_win_e = (freqs_ppg >= est_fs - fs_win) & (freqs_ppg <= est_fs + fs_win)\n",
    "        conf = np.sum(fft_val[fs_win_e])/np.sum(fft_val)\n",
    "        \n",
    "        error.append(float(np.abs((estimation - targets[i]))))\n",
    "        confid.append(conf)\n",
    "        \n",
    "    return np.array(error), np.array(confid)\n",
    "\n",
    "\n",
    "\n",
    "def BandpassFilter(signal, fs, order, passband):\n",
    "    \"\"\"\n",
    "    Band pass filter\n",
    "\n",
    "    Args:\n",
    "        signal: Signal to be filtered\n",
    "        fs: Sample frequency of the signal\n",
    "        order: Order of the filter\n",
    "        passband: A tuple containing the limits of the pass band as (min, max)\n",
    "        \n",
    "    Returns:\n",
    "        sp.signal.filtfilt(b, a, signal): Filtered signal\n",
    "    \"\"\"\n",
    "    b, a = sp.signal.butter(order, passband, btype='bandpass', fs=fs)\n",
    "    \n",
    "    return sp.signal.filtfilt(b, a, signal)\n",
    "\n",
    "\n",
    "def LoadRegressor(n_estimators, max_depth, features, targets, sub_id):\n",
    "    \"\"\"\n",
    "    Train and save a Random Forest Regressor\n",
    "\n",
    "     Args:\n",
    "        n_estimators: (number) The number of trees in the forest\n",
    "        max_tree_depth: (number) The maximum depth of the tree\n",
    "        features: (np.array) The feature matrix\n",
    "        targets: (np.array) Class labels\n",
    "        sub_id: (np.array) The subject id that the datapoint came from\n",
    "        \n",
    "    Returns:\n",
    "        Saved regressor as 'regression_model.joblib'\n",
    "    \"\"\"\n",
    "    \n",
    "    regression = AdaBoostRegressor(DecisionTreeRegressor(max_depth=max_depth), learning_rate=0.0001, n_estimators=n_estimators, random_state=42)\n",
    "    logo =  LeaveOneGroupOut() \n",
    "    \n",
    "    scores = []\n",
    "\n",
    "    for train_ind, test_ind in logo.split(features, targets.ravel(), sub_id):\n",
    "        X_train, y_train = features[train_ind], targets[train_ind]\n",
    "        X_test, y_test = features[test_ind], targets[test_ind]\n",
    "        \n",
    "        regression.fit(X_train, y_train.ravel())\n",
    "        y_pred = regression.predict(X_test)\n",
    "        score = mean_absolute_error(y_test.ravel(), y_pred)\n",
    "        scores.append(score)\n",
    "        \n",
    "        # save\n",
    "        joblib.dump(regression, 'regression_model.joblib')\n",
    "        \n",
    "\n",
    "def GenerateFeatures(subjects, signals, target, fs, window_length_s, window_shift_s):\n",
    "    \"\"\"Extract features from the dataset.\n",
    "    \n",
    "    Generate features by sliding a window across each dataset and computing\n",
    "    the features on each window.\n",
    "    \n",
    "    Args:\n",
    "    subjects: Features extracted from signals used for training the model\n",
    "    signals: The sampling rate of the data\n",
    "    target: Values extracted from reference data used as target value in supervised learning \n",
    "    fs: (number) Sample frequency\n",
    "    window_length_s: (number) The length of the window in seconds\n",
    "    window_shift_s: (number) The amount to shift the window by\n",
    "    \n",
    "    Returns:\n",
    "    targets: (np.array) Class labels\n",
    "    sub_id: (np.array) The subject id that the datapoint came from\n",
    "    features: (np.array) The feature matrix\n",
    "    samples: (np.array) Starting location of each window\n",
    "    \"\"\"\n",
    "    window_length = window_length_s * fs\n",
    "    window_shift = window_shift_s * fs\n",
    "    feature_ppg_peak, feature_ppg_peak, features_l2, features_x, features_y, features_z, targets, sub_id, samples = [], [], [], [], [], [], [], [], []  \n",
    "    mn_p, mn_l2, mn_x, mn_y, mn_z = [], [], [], [], []\n",
    "    \n",
    "    for index in range(len(signals)):\n",
    "\n",
    "        ppg, accx, accy, accz = signals[index]   \n",
    "        \n",
    "        filt_ppg = BandpassFilter(ppg, fs, 3, (1.67, 3.17))\n",
    "        filt_accx = BandpassFilter(accx, fs, 3, (1.67, 3.17))\n",
    "        filt_accy = BandpassFilter(accy, fs, 3, (1.67, 3.17))\n",
    "        filt_accz = BandpassFilter(accz, fs, 3, (1.67, 3.17))\n",
    "    \n",
    "        ppg_res = sp.signal.resample(filt_ppg, 300*fs)\n",
    "        accx_res = sp.signal.resample(filt_accx, 300*fs)\n",
    "        accy_res = sp.signal.resample(filt_accy, 300*fs)\n",
    "        accz_res = sp.signal.resample(filt_accz, 300*fs)\n",
    "    \n",
    "        \n",
    "        # Vector magnitude of the force on the accelerometer in 3D space\n",
    "        acc_l2 = np.sqrt(accx_res**2 + accy_res**2 + accz_res**2)\n",
    "        \n",
    "        target_res = sp.signal.resample(target[index], 146)\n",
    "        j = 0\n",
    "\n",
    "        for i in range(0, len(ppg_res) - window_length, window_shift):\n",
    "        \n",
    "            # Selection of the time window\n",
    "            ppg_ind = ppg_res[i: i + window_length]\n",
    "            acc_l2_ind = acc_l2[i: i + window_length]\n",
    "            accx_ind = accx_res[i: i + window_length]\n",
    "            accy_ind = accy_res[i: i + window_length]\n",
    "            accz_ind = accz_res[i: i + window_length]\n",
    "        \n",
    "            \n",
    "            mn_p.append(np.mean(ppg_ind))\n",
    "            mn_l2.append(np.mean(acc_l2_ind))\n",
    "            mn_x.append(np.mean(accx_ind))\n",
    "            mn_y.append(np.mean(accy_ind))\n",
    "            mn_z.append(np.mean(accz_ind))\n",
    "    \n",
    "            ppg_peak, acc_l2_peak = Featurize(ppg_ind, acc_l2_ind, fs=fs)\n",
    "            feature_ppg_peak.append(ppg_peak*60)\n",
    "            features_l2.append(ppg_peak*60)\n",
    "    \n",
    "            ppg_peak, acc_x_peak = Featurize(ppg_ind, accx_ind, fs=fs)\n",
    "            features_x.append(acc_x_peak*60)\n",
    "            \n",
    "            ppg_peak, acc_y_peak = Featurize(ppg_ind, accy_ind, fs=fs)\n",
    "            features_y.append(acc_y_peak*60)\n",
    "            \n",
    "            ppg_peak, acc_z_peak = Featurize(ppg_ind, accz_ind, fs=fs)\n",
    "            features_z.append(acc_z_peak*60)\n",
    "            \n",
    "            targets.append(target_res[j])\n",
    "            sub_id.append(subjects[index])\n",
    "            samples.append(i)\n",
    "            j += 1\n",
    "        \n",
    "    # Smooth the signals\n",
    "    features_ppg = savgol_filter(np.array(feature_ppg_peak), 51, 3) \n",
    "    features_l2 = savgol_filter(np.array(features_l2), 51, 3) \n",
    "    features_x = savgol_filter(np.array(features_x), 51, 3) \n",
    "    features_y = savgol_filter(np.array(features_y), 51, 3) \n",
    "    features_z = savgol_filter(np.array(features_z), 51, 3)\n",
    "    \n",
    "    features = np.array([features_ppg, features_l2, features_x, features_y, features_z, mn_p, mn_l2, mn_x, mn_y, mn_z]).T\n",
    "    targets = np.array(targets)\n",
    "    sub_id = np.array(sub_id)\n",
    "    \n",
    "    \n",
    "    return targets, sub_id, features, samples\n",
    "\n",
    "\n",
    "def Featurize(ppg, acc, fs):\n",
    "    \n",
    "    \"\"\"Featurization of the signals\n",
    "    \n",
    "    Args:\n",
    "    ppg: (np.array) ppg signal of the photoplethysmographic sensor\n",
    "    acc_l2: (np.array) Vector magnitude of the force on the accelerometer in 3D space\n",
    "    fs: (number) the sampling rate of the signals\n",
    "    \n",
    "    Returns:\n",
    "        ppg_peak: The frequency peak of the ppg signal\n",
    "        acc_peak: The frequency peak of the acc signal\n",
    "    \"\"\"\n",
    "    # The mean of each channel\n",
    "    freqs_ppg = np.fft.rfftfreq(len(ppg), 1/fs)\n",
    "    fft_val = np.abs(np.fft.rfft(ppg))\n",
    "    ppg_peak_freq = freqs_ppg[np.argmax(fft_val)]\n",
    "    \n",
    "    freqs_acc = np.fft.rfftfreq(len(acc), 1/fs)\n",
    "    fft_acc = np.abs(np.fft.rfft(acc))\n",
    "    acc_peak_freq = freqs_acc[np.argmax(fft_acc[10:])] # Ignore DC component\n",
    "\n",
    "    if np.abs(ppg_peak_freq - acc_peak_freq) <= 10:\n",
    "        ppg_peak_freq = freqs_ppg[np.argsort(fft_val, axis=0)][-2]\n",
    "    \n",
    "    ppg_peak = ppg_peak_freq\n",
    "    acc_peak = acc_peak_freq\n",
    "\n",
    "    return ppg_peak, acc_peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./datasets/troika/training_data/DATA_01_TYPE01.mat\n",
      "./datasets/troika/training_data/DATA_02_TYPE02.mat\n",
      "./datasets/troika/training_data/DATA_03_TYPE02.mat\n",
      "./datasets/troika/training_data/DATA_04_TYPE01.mat\n",
      "./datasets/troika/training_data/DATA_04_TYPE02.mat\n",
      "./datasets/troika/training_data/DATA_05_TYPE02.mat\n",
      "./datasets/troika/training_data/DATA_06_TYPE02.mat\n",
      "./datasets/troika/training_data/DATA_07_TYPE02.mat\n",
      "./datasets/troika/training_data/DATA_08_TYPE02.mat\n",
      "./datasets/troika/training_data/DATA_10_TYPE02.mat\n",
      "./datasets/troika/training_data/DATA_11_TYPE02.mat\n",
      "./datasets/troika/training_data/DATA_12_TYPE02.mat\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.6589510919563688"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Evaluate(125)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
